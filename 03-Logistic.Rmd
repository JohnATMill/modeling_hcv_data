---
output:
  pdf_document: default
  html_document: default
---

# LOGISTIC REGRESSION

```{r, include=FALSE}
library(glmnet)
library(verification)
```

## Data Partitioning

```{r}

data<-read.csv("data/CleanedData.csv",header = T,colClasses=c("NULL", rep(NA, 13))) 

set.seed(125)
V = 10
n = NROW(data); n0 = sum(data$Category==0); n1 = n-n0;
id.fold = 1:n
id.fold[data$Category==0] <- sample(x=1:V, size=n0, replace=TRUE)
id.fold[data$Category==1] <- sample(x=1:V, size=n1, replace=TRUE)
for (v in 1:V) {
train.v <- data[id.fold!=v, ]; test.v <- data[id.fold==v, ]

}

dim(train.v)
dim(test.v)
```

## 5-Fold Cross Validation - Logistic Regression

```{r, warning=FALSE, message=FALSE}
set.seed(666)
V <- 5
n <- NROW(data); n0 <- sum(data$Category==0); n1 <- n-n0;
missclass.rate = c()
err_vec1=c()
for (v in 1:V) {
  err_vec1=c(err_vec1, v)
  missclass.rate=c(missclass.rate, v)
}

id.fold <- 1:n
id.fold[data$Category==0] <- sample(x=1:V, size=n0, replace=TRUE)
id.fold[data$Category==1] <- sample(x=1:V, size=n1, replace=TRUE)


for (v in 1:V) {
train.v <- data[id.fold!=v, ]; test.v <- data[id.fold==v, ];

formula0 = Category~.
X = model.matrix (as.formula(formula0), data = train.v)
y = factor(train.v$Category)
fit.lasso = glmnet(x=X, y=y, family="binomial", alpha=1, 
                    lambda.min = 1e-4, nlambda = 100, standardize=T, thresh = 
                      1e-07, maxit=1000)

CV = cv.glmnet(x=X, y=y, family="binomial", alpha = 1,
               lambda.min = 1e-4, nlambda = 200, standardize = T,
               thresh = 1e-07, maxit=1000)

best.lambda = CV$lambda.1se; #best.lambda  
fit.best = glmnet(x=X, y=y, family="binomial", alpha = 1,
                  lambda=best.lambda, standardize = T, 
                  thresh = 1e-07, maxit=1000)

formula0 = Category ~. # ALP + AST + BIL + CHOL + CREA + GGT + PROT
fit.final = glm(formula0, family = "binomial", data = train.v)
#summary(fit.final)

yobs = test.v$Category
X.test = test.v[, -1]
pred.glm = predict(fit.final, newdata = X.test, type="response")

#X.test <- model.matrix (as.formula(formula0), data = test.v)
#pred.glm <- predict(fit.best, newx = X.test, s=best.lambda, type="response")

mod = roc.area(yobs, pred.glm)$A
err_vec1[v] = mod
print(paste("AUC for fold", v, ":", err_vec1[v]))

pred.rate = ifelse(pred.glm > 0.5, 1, 0)
miss.rate <- mean(yobs != pred.rate)
missclass.rate[v] = miss.rate
print(paste("Missclassification rate for fold", v,
           ":",missclass.rate[v]))

}
summary(fit.final)
```


```{r}
plot(CV)
```

*COMMENT*
We can see that the best parameter lambda is 


```{r}
Average.AUC.logis<-print(paste("Average of AUC:", mean(err_vec1)))
Average.miss.logis<-print(paste("Average of Miss:", mean(missclass.rate)))
print(fit.best$beta)
best.lambda
AUC.LOGIS<-mean(err_vec1)
miss.rate.LOIS<-mean(missclass.rate)

```


## Fitting the Best Model

```{r}
plot(fit.final)
#fit.best$beta
final.fit<-glm(train.v$Category~ALP+ALT+AST+BIL+CHOL+CREA+GGT+PROT, family = (link =  "binomial"), data = train.v)

```



**comment**

Cross validation is used to select the best tuning parameter for the logistic regression.We can observe that the best lambda is 0.0204.We can also observe that,the coefficient(ALP,AST and CHOL) are all negative and thus have been shrunk to 0.Thus,we are left with the variables with the positive values.Also,we can observe that all the values the parameters with positive coefficient are statistically significant.

*Average of AUC and Missclassification Rate;*

We can observe that the average of the AUC from the V-folds is approximately 0.96 and misclassification of 0.04.This shows that model was good in making correct with predictions.




