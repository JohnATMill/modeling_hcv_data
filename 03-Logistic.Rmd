
# REGULARIZED LOGISTIC REGRESSION  

```{r, include=FALSE}
library(glmnet)
library(verification)
```

## Data Partition

```{r}

data<-read.csv("data/CleanedData.csv",header = T,colClasses=c("NULL", rep(NA, 13))) 

set.seed(125)
V = 5
n = NROW(data); n0 = sum(data$Category==0); n1 = n-n0;
id.fold = 1:n
id.fold[data$Category==0] <- sample(x=1:V, size=n0, replace=TRUE)
id.fold[data$Category==1] <- sample(x=1:V, size=n1, replace=TRUE)
for (v in 1:V) {
train.v <- data[id.fold!=v, ]; test.v <- data[id.fold==v, ]

}

dim(train.v)
dim(test.v)
```

## 5-Fold Cross Validation - Logistic Regression

```{r, warning=FALSE, message=FALSE}
set.seed(666)
V <- 5
n <- NROW(data); n0 <- sum(data$Category==0); n1 <- n-n0;
missclass.rate = c()
err_vec1=c()
for (v in 1:V) {
  err_vec1=c(err_vec1, v)
  missclass.rate=c(missclass.rate, v)
}

id.fold <- 1:n
id.fold[data$Category==0] <- sample(x=1:V, size=n0, replace=TRUE)
id.fold[data$Category==1] <- sample(x=1:V, size=n1, replace=TRUE)



#Model, Error Rate, Prediction, AUC
for (v in 1:V) {
train.v <- data[id.fold!=v, ]; test.v <- data[id.fold==v, ];

formula0 = Category~.
X = model.matrix (as.formula(formula0), data = train.v)
y = factor(train.v$Category)
fit.lasso = glmnet(x=X, y=y, family="binomial", alpha=1, 
                    lambda.min = 1e-4, nlambda = 100, standardize=T, thresh = 
                      1e-07, maxit=1000)

CV = cv.glmnet(x=X, y=y, family="binomial", alpha = 1,
               lambda.min = 1e-4, nlambda = 200, standardize = T,
               thresh = 1e-07, maxit=1000)

best.lambda = CV$lambda.1se; #best.lambda  
fit.best = glmnet(x=X, y=y, family="binomial", alpha = 1,
                  lambda=best.lambda, standardize = T, 
                  thresh = 1e-07, maxit=1000)

formula0 = Category ~. 
fit.final = glm(formula0, family = "binomial", data = train.v)

#Observed response
yobs = test.v$Category
X.test = test.v[, -1]
pred.glm = predict(fit.final, newdata = X.test, type="response")


#AUC

mod = roc.area(yobs, pred.glm)$A
err_vec1[v] = mod
print(paste("AUC for fold", v, ":", err_vec1[v]))

pred.rate = ifelse(pred.glm > 0.5, 1, 0)
miss.rate <- mean(yobs != pred.rate)
missclass.rate[v] = miss.rate
print(paste("Missclassification rate for fold", v,
           ":",missclass.rate[v]))

}
summary(fit.final)
```


```{r}
plot(CV)
plot(mod)
```

 


```{r}
Average.AUC.logis<-print(paste("Average of AUC:", mean(err_vec1)))
Average.miss.logis<-print(paste("Average of Miss:", mean(missclass.rate)))

AUC.LOGIS<-mean(err_vec1)
miss.rate.LOGIS<-mean(missclass.rate)

```


## Fitting the Best Model

```{r, warning=FALSE}
plot(fit.final)
#fit.best$beta
final.fit<-glm(train.v$Category~ALP+ALT+AST+BIL+CHOL+CREA+GGT+PROT, family = (link =  "binomial"), data = train.v)

```




Cross validation is used to select the best tuning parameter for the logistic regression. Obtain best tuning parameters - also, unimportant coefficients are shrunk to 0. Fit.final output the remaining significant variables.

*Average of AUC and Missclassification Rate;*

**Note** 
^[Logistic AUC: 0.98, MisClassification Rate: 0.027]


