[["logistic-regression.html", "3 LOGISTIC REGRESSION 3.1 Data Partitioning 3.2 5-Fold Cross Validation - Logistic Regression 3.3 Fitting the Best Model", " 3 LOGISTIC REGRESSION 3.1 Data Partitioning data&lt;-read.csv(&quot;data/CleanedData.csv&quot;,header = T,colClasses=c(&quot;NULL&quot;, rep(NA, 13))) set.seed(125) V = 10 n = NROW(data); n0 = sum(data$Category==0); n1 = n-n0; id.fold = 1:n id.fold[data$Category==0] &lt;- sample(x=1:V, size=n0, replace=TRUE) id.fold[data$Category==1] &lt;- sample(x=1:V, size=n1, replace=TRUE) for (v in 1:V) { train.v &lt;- data[id.fold!=v, ]; test.v &lt;- data[id.fold==v, ] } dim(train.v) ## [1] 553 13 dim(test.v) ## [1] 62 13 3.2 5-Fold Cross Validation - Logistic Regression set.seed(666) V &lt;- 5 n &lt;- NROW(data); n0 &lt;- sum(data$Category==0); n1 &lt;- n-n0; missclass.rate = c() err_vec1=c() for (v in 1:V) { err_vec1=c(err_vec1, v) missclass.rate=c(missclass.rate, v) } id.fold &lt;- 1:n id.fold[data$Category==0] &lt;- sample(x=1:V, size=n0, replace=TRUE) id.fold[data$Category==1] &lt;- sample(x=1:V, size=n1, replace=TRUE) for (v in 1:V) { train.v &lt;- data[id.fold!=v, ]; test.v &lt;- data[id.fold==v, ]; formula0 = Category~. X = model.matrix (as.formula(formula0), data = train.v) y = factor(train.v$Category) fit.lasso = glmnet(x=X, y=y, family=&quot;binomial&quot;, alpha=1, lambda.min = 1e-4, nlambda = 100, standardize=T, thresh = 1e-07, maxit=1000) CV = cv.glmnet(x=X, y=y, family=&quot;binomial&quot;, alpha = 1, lambda.min = 1e-4, nlambda = 200, standardize = T, thresh = 1e-07, maxit=1000) best.lambda = CV$lambda.1se; #best.lambda fit.best = glmnet(x=X, y=y, family=&quot;binomial&quot;, alpha = 1, lambda=best.lambda, standardize = T, thresh = 1e-07, maxit=1000) formula0 = Category ~. # ALP + AST + BIL + CHOL + CREA + GGT + PROT fit.final = glm(formula0, family = &quot;binomial&quot;, data = train.v) #summary(fit.final) yobs = test.v$Category X.test = test.v[, -1] pred.glm = predict(fit.final, newdata = X.test, type=&quot;response&quot;) #X.test &lt;- model.matrix (as.formula(formula0), data = test.v) #pred.glm &lt;- predict(fit.best, newx = X.test, s=best.lambda, type=&quot;response&quot;) mod = roc.area(yobs, pred.glm)$A err_vec1[v] = mod print(paste(&quot;AUC for fold&quot;, v, &quot;:&quot;, err_vec1[v])) pred.rate = ifelse(pred.glm &gt; 0.5, 1, 0) miss.rate &lt;- mean(yobs != pred.rate) missclass.rate[v] = miss.rate print(paste(&quot;Missclassification rate for fold&quot;, v, &quot;:&quot;,missclass.rate[v])) } ## [1] &quot;AUC for fold 1 : 0.978378378378378&quot; ## [1] &quot;Missclassification rate for fold 1 : 0.0458015267175573&quot; ## [1] &quot;AUC for fold 2 : 0.98735119047619&quot; ## [1] &quot;Missclassification rate for fold 2 : 0.0403225806451613&quot; ## [1] &quot;AUC for fold 3 : 0.965210355987055&quot; ## [1] &quot;Missclassification rate for fold 3 : 0.0260869565217391&quot; ## [1] &quot;AUC for fold 4 : 0.99609375&quot; ## [1] &quot;Missclassification rate for fold 4 : 0.03125&quot; ## [1] &quot;AUC for fold 5 : 0.858169934640523&quot; ## [1] &quot;Missclassification rate for fold 5 : 0.0512820512820513&quot; summary(fit.final) ## ## Call: ## glm(formula = formula0, family = &quot;binomial&quot;, data = train.v) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.7241 -0.1364 -0.0548 -0.0166 2.4841 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -17.923282 5.700656 -3.144 0.001666 ** ## Age 0.016376 0.029259 0.560 0.575685 ## Sexm -0.739067 0.754738 -0.979 0.327463 ## ALB -0.187535 0.081815 -2.292 0.021896 * ## ALP -0.097945 0.021681 -4.517 6.26e-06 *** ## ALT -0.019738 0.012560 -1.572 0.116062 ## AST 0.087722 0.021603 4.061 4.90e-05 *** ## BIL 0.080003 0.033607 2.381 0.017287 * ## CHE 0.130583 0.162598 0.803 0.421915 ## CHOL -1.143580 0.367336 -3.113 0.001851 ** ## CREA 0.027751 0.007481 3.709 0.000208 *** ## GGT 0.031631 0.007223 4.379 1.19e-05 *** ## PROT 0.355355 0.096086 3.698 0.000217 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 366.413 on 497 degrees of freedom ## Residual deviance: 83.104 on 485 degrees of freedom ## AIC: 109.1 ## ## Number of Fisher Scoring iterations: 8 plot(CV) COMMENT We can see that the best parameter lambda is Average.AUC.logis&lt;-print(paste(&quot;Average of AUC:&quot;, mean(err_vec1))) ## [1] &quot;Average of AUC: 0.957040721896429&quot; Average.miss.logis&lt;-print(paste(&quot;Average of Miss:&quot;, mean(missclass.rate))) ## [1] &quot;Average of Miss: 0.0389486230333018&quot; print(fit.best$beta) ## 13 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) . ## Age . ## Sexm . ## ALB . ## ALP -0.035592285 ## ALT . ## AST 0.041589191 ## BIL 0.019166617 ## CHE . ## CHOL -0.393023862 ## CREA 0.004741066 ## GGT 0.012544798 ## PROT 0.045586429 best.lambda ## [1] 0.01572112 AUC.LOGIS&lt;-mean(err_vec1) miss.rate.LOIS&lt;-mean(missclass.rate) 3.3 Fitting the Best Model plot(fit.final) #fit.best$beta final.fit&lt;-glm(train.v$Category~ALP+ALT+AST+BIL+CHOL+CREA+GGT+PROT, family = (link = &quot;binomial&quot;), data = train.v) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred comment Cross validation is used to select the best tuning parameter for the logistic regression.We can observe that the best lambda is 0.0204.We can also observe that,the coefficient(ALP,AST and CHOL) are all negative and thus have been shrunk to 0.Thus,we are left with the variables with the positive values.Also,we can observe that all the values the parameters with positive coefficient are statistically significant. Average of AUC and Missclassification Rate; We can observe that the average of the AUC from the V-folds is approximately 0.96 and misclassification of 0.04.This shows that model was good in making correct with predictions. "]]
